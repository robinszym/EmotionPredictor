{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4981a184-62ba-44e1-89c1-ce0b031851a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset creation\n",
    "\n",
    "This notebook extracts the features from the ArtEmis dataset for every architecture used in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bc54e7f3-3786-4dbd-8141-731129a22215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'data_tools' from '/scratch/students/2021-fall-mt-rszymcza/code/data_tools.py'>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import copy\n",
    "import argparse\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import ast\n",
    "import data_tools as dt\n",
    "import torch.nn as nn\n",
    "import custom as ct\n",
    "\n",
    "from tqdm import tqdm\n",
    "from artemis.emotions import ARTEMIS_EMOTIONS  \n",
    "imp.reload(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "201eb757-b727-4661-95fa-9ef876fe6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "data_path = \"../_data/\" #Change to the directory you want the embeddings to be saved\n",
    "\n",
    "# The CSVs can be found at https://www.artemisdataset.org/#dataset\n",
    "# And follow the steps at https://github.com/optas/artemis\n",
    "image_emotion_histogram_path = \"../code/artemis/artemis/data/image-emotion-histogram.csv\" \n",
    "artemis_preprocessed_path = \"artemis/artemis/data/artemis_preprocessed.csv\" \n",
    "\n",
    "# To download the wikiart dataset : http://web.fsktm.um.edu.my/~cschan/source/ICIP2017/wikiart.zip\n",
    "wikiart_path = \"../data/wikiart/\"\n",
    "\n",
    "preprocessed_dir = osp.join(data_path, \"preprocessed\")\n",
    "if not osp.exists(preprocessed_dir) : os.mkdir(preprocessed_dir)\n",
    "\n",
    "wikiart_embeddings_path =  osp.join(data_path, \"wikiart_embeddings\")\n",
    "if not osp.exists(wikiart_embeddings_path) : os.mkdir(wikiart_embeddings_path)\n",
    "\n",
    "clip_path =  osp.join(wikiart_embeddings_path, \"clip\")\n",
    "if not osp.exists(clip_path) : os.mkdir(clip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48005e13-7413-4846-a8e9-69e576faa721",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CLIP Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a692cf1-04ed-438f-8787-d83c04ef9020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee95213-3b48-4ebd-8c33-23690eb8cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EigenModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "def dataset_for_subsets( model, loaders_dict, path,recreate = False):\n",
    "    \"Create a dataset for each subset dictionary of data_loaders\"\n",
    "    if not osp.exists(path) : os.mkdir(path)\n",
    "    for subset, loader in loaders_dict.items():\n",
    "        dt.create_dataset(model,\n",
    "                          loader,\n",
    "                          osp.join(path, subset),\n",
    "                          recreate = recreate)\n",
    "\n",
    "for model_name in clip.available_models():\n",
    "    model, preprocess = clip.load(model_name)\n",
    "    model = model.visual.to(device)\n",
    "    high_res = model_name in high_res_models\n",
    "    img_size = preprocess.transforms[1].size[0]\n",
    "    path = osp.join(preprocessed_dir, f\"img_size_{img_size}\")\n",
    "    if not osp.exists(path) : \n",
    "        os.mkdir(path)\n",
    "        wikiart_loaders = dt.get_original_wikiart_dataloaders(image_emotion_histogram_path,\n",
    "                                          artemis_preprocessed_path,\n",
    "                                          wikiart_path,\n",
    "                                          preprocess)\n",
    "        dataset_for_subsets(EigenModel(),\n",
    "                          wikiart_loaders,\n",
    "                          path)\n",
    "        \n",
    "    \n",
    "    img_loaders = dt.get_loaders(path)     \n",
    "    model_name = model_name.replace(\"/\", \"\")\n",
    "    print(f\"creating set for {model_name}\")\n",
    "    model_path = osp.join(clip_path, model_name)\n",
    "    dataset_for_subsets(model,\n",
    "                          img_loaders,\n",
    "                          model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f86427-af6a-4490-a378-f62ba33867e3",
   "metadata": {},
   "source": [
    "## ImageNet pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "62185d1d-073f-401c-b69c-ddeec6796446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "70ba220d-f49d-4301-8873-bdad2291d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_224 = osp.join(preprocessed_dir, \"img_size_224\")\n",
    "loaders_224 = dt.get_loaders(path_224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eda584cd-dece-4aeb-85d9-e6c32b1b3c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_imagenet = {}\n",
    "models_imagenet[\"resnet50\"] = models.resnet50(pretrained = True)\n",
    "models_imagenet[\"alexnet\"]= models.alexnet(pretrained = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5d643e67-98a1-4b34-aced-207724e503d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imagenet_embeddings = osp.join(wikiart_embeddings_path, \"imagenet\")\n",
    "if not osp.exists(path_imagenet_embeddings) : os.mkdir(path_imagenet_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c7c88-26c7-4c3f-ae1b-dce87290e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models_imagenet.items():\n",
    "    path = osp.join(path_imagenet_embeddings, name)\n",
    "    if not osp.exists(path) : os.mkdir(path)\n",
    "    dataset_for_subsets(model,\n",
    "                          loaders_224,\n",
    "                          path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
