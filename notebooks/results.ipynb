{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e05cd4-ece6-4685-b08f-f600409f4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import emotionpredictor.data_tools as dt\n",
    "from emotionpredictor import evaluation\n",
    "from emotionpredictor.evaluation import Report\n",
    "from emotionpredictor.visual import show_torch_image\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f43dd20e-d528-4c03-81cf-760556df6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e8f8718-2c71-42a4-b962-bb4c5d3245ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTEMIS_EMOTIONS = ['amusement',\n",
    " 'awe',\n",
    " 'contentment',\n",
    " 'excitement',\n",
    " 'anger',\n",
    " 'disgust',\n",
    " 'fear',\n",
    " 'sadness',\n",
    " 'something else']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8137350",
   "metadata": {},
   "source": [
    "<b>Download the images</b>\n",
    "\n",
    "1. You can download the test set required for visualisation at this [link](https://www.dropbox.com/s/ybn5l1rhx4zzyuy/test_set_artemis_224.zip?dl=0).\n",
    "\n",
    "2. After unzipping the file, update the <code>img_folder_path</code> path of the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13aa6f50-a8c3-4a4c-aa1f-398fca7739fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_img_folder = \"../_data/preprocessed/\"\n",
    "preprocessed_img_folder = osp.join(preprocessed_img_folder, \"img_size_224\")\n",
    "\n",
    "preprocessed_img_folder = \"../data/test_set_artemis_224/\"\n",
    "test_results_path = \"../artemis_testset_results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df321c-041c-4dd5-a6c9-3105f82c44fb",
   "metadata": {},
   "source": [
    "## Colect results of every model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c10f5485-3c68-4f2f-9543-0d063ca635bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = {}\n",
    "for name in os.listdir(test_results_path):\n",
    "    report = Report(class_labels=ARTEMIS_EMOTIONS)\n",
    "    report.load_labels_results(osp.join(test_results_path, name))\n",
    "    reports[name] = report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5254e95a-491c-4d71-b14b-6fe364cd8a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d10b866b6134f64b07f11b666b115db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('A-resnet50', 'C-RN101', 'C-RN50', 'C-RN50x16', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys_ordered = list(reports.keys())\n",
    "keys_ordered.sort()\n",
    "@interact(model = keys_ordered , threshold = (0.5, 1, 0.05))\n",
    "def show_reports(model, threshold):\n",
    "    reports[model].threshold = threshold\n",
    "    reports[model].show_results(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78034a16-375e-4cfb-b770-384eacc89d19",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overall metrics comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dba5a866-1122-4f12-8c92-b4723b55b60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2bb537d3d164441ada813fa1eede92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.75, description='threshold', max=1.0, min=0.5, step=0.05), Output())…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(threshold = (0.5, 1, 0.05))\n",
    "def overall_metrics(threshold):\n",
    "    overall = []\n",
    "    \n",
    "    for name, model in reports.items():\n",
    "        model.threshold = threshold\n",
    "        overall.append([name, model.overall_metrics()])\n",
    "    keys, values = zip(*overall)\n",
    "    df_overall = pd.concat(values, axis = 1)\n",
    "    df_overall.columns = keys\n",
    "    fig = px.bar(df_overall[keys_ordered], barmode = \"group\", height = 500)\n",
    "    #fig.update_layout(transition_duration = 1500)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05658969-c617-478d-a8f5-7dce0f3a4f74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Paintings maximising an emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80652b1a-0eb5-4ae5-b499-61087d2fde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_loader = dt.Pickle_data_loader(osp.join(preprocessed_img_folder), )\n",
    "emotion2index = dict(zip(ARTEMIS_EMOTIONS, range(9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d037c2e6-e2f0-4a88-8f32-c897a441c49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b3d683bc494dcbaf6bbb66e6dca842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=True, description='relative'), Dropdown(description='emotion', options=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "@interact(model = keys_ordered, emotion = ARTEMIS_EMOTIONS, relative = True, n = (2,5,1))\n",
    "def paintings_maximising_emotions(relative, emotion, model, n):\n",
    "\n",
    "    fig, axs = plt.subplots(1, n, figsize=(16,9))\n",
    "    model = reports[model]\n",
    "    res = model.results\n",
    "\n",
    "    res_sf = torch.tensor(res).softmax(1).numpy()\n",
    "    max_emotions_paintings = {}\n",
    "    if relative: \n",
    "        id_max = np.argsort(res_sf[:,emotion2index[emotion]])[-n:]\n",
    "    else : \n",
    "        id_max = np.argsort(res[:,emotion2index[emotion]])[-n:]\n",
    "        \n",
    "    for i, index in enumerate(id_max):\n",
    "        axis = axs[i]\n",
    "        if i == 0 :axis.title.set_text(f\"{emotion.capitalize()}: {round(res_sf[index, emotion2index[emotion]]*100)}%\")\n",
    "        else : axis.title.set_text(f\"{round(res_sf[index, emotion2index[emotion]]*100)}%\")\n",
    "        show_torch_image(image_loader.load_label(index, 40)[\"image\"], axis = axis)\n",
    "        axis.set_ylabel(emotion.capitalize())\n",
    "\n",
    "    #save_fig(f\"../results/clip_training/{model}/maxed_{emotion}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b16518-9a1c-4417-b850-0be33245d0f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21f9728e-cd90-483c-aaaa-6c738b1eef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baae654a6684371bb3852957f246c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='model', options=('A-resnet50', 'C-RN101', 'C-RN50', 'C-RN50x16', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "@interact(model = keys_ordered, n = (2,4,1))\n",
    "def failures(model, n):\n",
    "    model = reports[model]\n",
    "    results = model.results\n",
    "    labels = model.labels\n",
    "    argmax = results.argmax(1)\n",
    "    argmax = torch.nn.functional.one_hot(torch.tensor(argmax),9).numpy().astype(bool)\n",
    "    results_off = labels[argmax]\n",
    "    results_softmax = torch.tensor(results).softmax(1).numpy()\n",
    "    high_confidence_pred = (results_softmax.max(1)>0.65)\n",
    "    SUBSET_SIZE = n**2\n",
    "    N_HORIZONTAL = n\n",
    "    \n",
    "    subset_ids = np.where((results_off == 0) & high_confidence_pred)[0]#[:SUBSET_SIZE]\n",
    "    subset_ids = np.where((results_off == 0))[0][:SUBSET_SIZE]\n",
    "    fig, axs = plt.subplots(N_HORIZONTAL, N_HORIZONTAL, figsize = (5+n*3,5+n*3))\n",
    "    for i, label in enumerate(subset_ids):\n",
    "        expected_emotion = ARTEMIS_EMOTIONS[labels[label].argmax()]\n",
    "        expected_confidence = np.round(labels[label].max()*100, 1)\n",
    "\n",
    "        predicted_emotion = ARTEMIS_EMOTIONS[results[label].argmax()]\n",
    "        predicted_confidence = torch.tensor(results[label]).softmax(0).max().numpy()\n",
    "        predicted_confidence = np.round(predicted_confidence*100,1)\n",
    "        visual = image_loader.load_label(label, 40)[\"image\"]\n",
    "        axis = axs[divmod(i, N_HORIZONTAL)]\n",
    "        show_torch_image(image_loader.load_label(label, 40)[\"image\"], axis = axis)\n",
    "        axis.set_title(f\"Expected : {expected_emotion} ({expected_confidence}%)\\n Predicted : {predicted_emotion} ({predicted_confidence}%)\")\n",
    "\n",
    "    #fig.savefig(\"../results/visuals/failure1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26662aca-931e-4dd9-a552-608d0b824b5c",
   "metadata": {},
   "source": [
    "## Browse test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea78bc4-0a63-4df9-931f-02c446f735da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n = 40\n",
    "@torch.no_grad()\n",
    "@interact(model = keys_ordered, emotion=ARTEMIS_EMOTIONS, n=(0, max_n-1,1))\n",
    "def highest_confidence_paintings(model, n, emotion):\n",
    "    model = reports[model]\n",
    "    results_softmax = torch.tensor(model.results).softmax(1).numpy()\n",
    "    plt.figure(figsize = (8,8))\n",
    "    values, labels = torch.tensor(model.labels[:,emotion2index[emotion]]).topk(max_n)\n",
    "    show_torch_image(image_loader.load_label(labels[n].tolist(), batch_size=40)[\"image\"])\n",
    "    plt.title(\n",
    "        f\"expected : {values[n].tolist()*100:.1f}%\\n predicted {results_softmax[labels[n],emotion2index[emotion]]*100:.1f}%\" )\n",
    "\n",
    "#save_fig(\"../results/clip_training/RN50x16/error_disgust.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd26e7-6c54-44b1-989b-2ad689e4706b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Top n and effect of threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03383c0-1194-4a6a-8700-4e937dff2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_in_top_n(results, labels, n, threshold = None, n_labels = 1):\n",
    "    labels_max = copy.copy(labels)\n",
    "    labels_max.sort(1)\n",
    "\n",
    "    label_set = [np.arange(0,9)[arg >= v[-n_labels]].tolist() for v, arg in zip(labels_max, labels)]\n",
    "    \n",
    "    c = pd.DataFrame([results.argsort()[:,-n:].tolist(), label_set]).T\n",
    "    if bool(threshold):\n",
    "        # keep only the histograms with the sum of the n max above a threshold\n",
    "        size = len(c.index)\n",
    "        c = c[threshold_sum_n(labels, threshold, n)]\n",
    "        #print(f\"Kept {round(100 * len(c.index)/size)}% of the set\")\n",
    "    c = c.applymap(lambda y : set(y))\n",
    "    return c.apply(lambda y : len(y[0].intersection(y[1])), axis=1)\n",
    "\n",
    "\n",
    "def threshold_sum_n(array, thresh, n):\n",
    "    if n == 1 :\n",
    "        return np.sort(array ,1)[:,-n:] > thresh\n",
    "    return np.sort(array ,1)[:,-n:].sum(1) > thresh\n",
    "    \n",
    "\n",
    "def results_df_top_n(n, threshold, report = None, results = None, labels = None, n_labels = 1):\n",
    "    assert n > 0 , f\"Please give a proper n, you inputed {n} -.-\" \n",
    "    if report is not None :\n",
    "        labels = report[\"labels\"]\n",
    "        results = report[\"results\"]\n",
    "    assert labels is not None, \"Please provide the model data or the labels\"\n",
    "    assert results is not None, \"Please provide the model data or the results\"\n",
    "    top_n = results_in_top_n(results, labels, n, threshold, n_labels = n_labels).value_counts()\n",
    "    top_n = pd.DataFrame((top_n*100/(top_n.sum())).round(2))\n",
    "    return top_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad1dfab-39df-472a-a7fd-1cccb13db2fe",
   "metadata": {},
   "source": [
    "## Top 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d8e9bb-ad81-4513-a820-f487f9fe0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_max = copy.copy(labels)\n",
    "labels_max.sort(1)\n",
    "labels_argmax = copy.copy(labels)\n",
    "labels_argmax.argsort(1)\n",
    "label_set = [np.arange(0,9)[arg >= v[-n_labels]].tolist() for v, arg in zip(labels_max, labels) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cbf147-6f09-457a-a7a4-c063b5faf4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slps_top2 = pd.DataFrame(index = reports.keys(), columns = np.arange(0, 1.0, 0.05))\n",
    "for key, slp in tqdm(reports.items()) :\n",
    "    for thresh in np.arange(0, 1.0, 0.05):\n",
    "        slps_top2[thresh].loc[key] = results_df_top_n(2,thresh, n_labels=2, report = slp)[0]\n",
    "        break      \n",
    "top2s = pd.concat(slps_top2[0.00].to_list(), axis=1)\n",
    "top2s.columns = slps_top2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba4e2f-a9b8-45af-a163-38036057350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.color_palette(\"pastel\")\n",
    "axs = top2s[keys_ordered].T[[2,1,0]].plot.bar(stacked = True,\n",
    "                        figsize=(9,9),\n",
    "                        color= [p[2], p[0], p[3]],\n",
    "                       #hatch = True,\n",
    "\n",
    "                       )\n",
    "\n",
    "plt.grid(axis= \"y\", ls = \":\", color = \"black\")\n",
    "plt.legend([\"Two matches\", \"One match\", \"No match\"])\n",
    "plt.ylim(0,100)\n",
    "plt.yticks(range(0,105,5));\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel(\"[%]\");\n",
    "#save_fig(\"../results/figures/top2_small.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ef1ad-8f01-4d1c-b288-36ae7b18a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.color_palette(\"pastel\" )\n",
    "ppx, _ = px.colors.convert_colors_to_same_type(p)\n",
    "px.bar(top2s[keys_ordered].T[[2,1,0]],\n",
    "       color_discrete_sequence=[ppx[2], ppx[0], ppx[3]], height = 600\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced6bc1-51af-4dca-892b-b515de817d57",
   "metadata": {},
   "source": [
    "## Accuracy as a function of annotators' agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e810223-8c7e-4605-b34f-e8785d9524df",
   "metadata": {},
   "outputs": [],
   "source": [
    "slps_top1 = pd.DataFrame(index = reports.keys(), columns = np.arange(0, 1.0, 0.05))\n",
    "for key, slp in tqdm(reports.items()) :\n",
    "    for thresh in np.arange(0, 1.0, 0.05):\n",
    "        slps_top1[thresh].loc[key] = results_df_top_n(1,thresh, n_labels=1, report = slp)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3dd14-9dc4-40b8-b815-288b002940a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "slps_plot = slps_top1.applymap(lambda y: y.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e3806-383e-4598-9d72-8be744e54cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(slps_plot.T, height = 600, range_y = (0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de36e7-87cf-4bce-8b73-e556b09e7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context = \"notebook\", style = \"white\")\n",
    "slps_top1.applymap(lambda y :  y.loc[1]).T.plot(figsize = (16,9))\n",
    "plt.legend()\n",
    "plt.ylim(0,100)\n",
    "plt.yticks(range(0,100,5));\n",
    "plt.xlabel(\"Agreement threshold\")\n",
    "plt.ylabel(\"Accuracy [%]\")\n",
    "plt.xlim(0,0.96)\n",
    "plt.grid(ls = \":\")\n",
    "#save_fig(\"../results/figures/top1_all.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
